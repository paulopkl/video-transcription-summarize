 Nesse vídeo, vamos explorar um modelo diferente. Modelo capaz de transcriver audio. Estou falando do Whisperer. Whisperer um modelo opensóis, mantido pela Opniai, que também está disponível para ser consumido via API. Esse modelo multilíngui além de fazer transcrição também consegue traduzir outros idiomas para inglês. Por esse vídeo, pensei um exemplo bem interessante, um sumarizador de vídeos do YouTube usando Whisperer e Langshan. Como já vimos alguns recursos do Langshan em outros vídeos, como Shans, Promptes e Memória, suponho que você estava familiarizado. Vamos dar uma olhada no design do Projeto para entender o que faremos nesse exemplo. Primeiro baixamos o vídeo do YouTube de nosso interesse e o transcrivamos usando Whisperer. Em seguida, vamos criar resumos usando duas abordagens diferentes. Primeiro utilizando a Shans, sumarização existente para gerar o resumo final que trata automaticamente em bérios e prómpicos. Depois, usamos outra abordagem mais detalhada para gerar um resumo final formatado em tópicos ou bullet points. O plano é dividir a transcrição em Txanx, gerar em bérios e preparar prómpicos. O próximo passo é adicionar aí chaves de API para os serviços da OpenAI Quadrant nas variáveis de um ambiente. E eu já fiz isso, então vou carregar. Aqui nós temos uma função que vai baixar o Arque VMP4 do YouTube na melhor qualidade e vai salvar o Arque localmente. Essa função recebe com margumento, um URL, essa é a versão mais rápida de implementar. Vai basear nessa biblioteca e o YouTube DLP que facilita bastante e trata todas as abistrações na hora de fazer o download de arquivos de vídeo ou transformar em arquivos de audio. Para esse experimento, vou pegar um vídeo aqui do canal. Você pode escolher qualquer outro se preferir. Nesse vídeo eu falo um pouco sobre Word Embarrass e busca o possibilidade. Vamos testar, vai demorar um tempinho. Então eu volto assim que conclui. E funciona. Se a gente der um olhar daqui na raiz do projeto, temos o arquivo em Fuselac MP4. Então só para conferir MP4211 megawatt. Aqui eu tenho uma outra versão refatorada para extrair apenas o audio. Que é o que importa para o isper. Nos parametrozeus escolher a qualidade do audio que vai ser extraído bem como o codec. Esse caso é MP3. Vamos testar outra vez o executar e funciona muito bem. Comparando com o MP4, o MP3 é bem menor. Então facilita na hora de trabalhar. Então vamos usar esse arquivo de MP3. Agora é a vez do isper. Como falei no início, o isper é um sistema de reconhecimento automático de audio bastante avançado, desenvolvido pela OpenAI. Esse modelo foi treinado e impressionante. 680 mil horas de dados, supervisionados, multilínguis e multitarefas, extraídos da web. Esse conjunto de dados vasto e variado aumenta bastante a capacidade do sistema, permitindo que ele iria facilmente com sótax, uidos de fundo, a linguagem técnica. O pacote isper, que eu tenho instalado localmente, fornece o método Lord Model. Para baixar o modelo utilizado para transcrever um arquivo de vídeo ou o audio. Diversos tipos de modelos diferentes estão disponíveis. Se a gente dá uma conferida na documentação, extremos o TNB, smal, midium e large. Cada um deles tem compensações entre precisão e velocidade. Vamos utilizar o modelo base para esse tutorial. Por que eu poder de processamento dessa máquina que eu estou usando aqui? Para gravar esse vídeo não é tão grande, mas se você tiver um GPU com memória suficiente para essa tarefa, a operação vai ficar bem mais rápida. Vou mostrar como utilizar o mesmo modelo pela API, que é o que provavelmente você vai utilizar em produção. Ok, antes de executar, eu vou botar a função special time para a gente comparar o tempo com a execução das APIs, só para ter o resistro da execução local. Vai levar um tempo para concluir essa tarefa e eu volto quando finalizar. Ok, localmente eu levou um minuto e oito segundos, que eu posso dar uma olhada no resultado da transcrição, então eu vou imprimir o result e apenas o texto. Como é possível ver com esse modelo base utilizado? A transcrição não fica tão boa, ele é erra algumas palavras. Quero para começar com o Word Embarrass, são a forma de representar palavras e textos como vetores numericos. Ele transcriveu como o Orden Bearings. Ficou bem estranho. A curácia na transcrição do modelo base não é tão boa para português. Vou fazer a mesma coisa, utilizando a API da UPNAI, então eu vou passar o arquivo de áudio, vou fazer o API da este arquivo de áudio, de 10 MHz, para API e fazer a transcrição com o modelo Isper One. Seguindo as instruções da documentação da UPNAI. Aqui eu não estou utilizando o link, é uma requisação direta na API da UPNAI. Em comparação com a execução local pela API da UPNAI, levou 41 segundos, versus 1 minuto e 8 segundos. Um pouco mais rápida. Vou salvar o conteúdo da transcrição, é um arquivo de texto, chamado o text.txt, aqui nós podemos dar uma conferida no resultado, e comparar a qualidade da transcrição via API com o modelo que eu utilizei localmente. E a qualidade da transcrição via API já foi bem melhor, porque o modelo utilizado é um pouco mais robusto na API. Já começa com a palavra correta, o Word Embarrass. Pensando em velocidade, a gente também pode fazer um experimento com o Grok. Na infraestrutura do Grok, ele suporta via API, o modelo Isper, o lado da UPNAI, usando a versão dele mais robusta, com um poder de transcrição de maior qualidade. Então vou fazer a mesma coisa, utilizar no Grok. Coleta o tempo e a gente compara com a API direta da UPNAI e o local. Em comparação com a API da UPNAI, pelo Grok, pela infraestrutura do Grok, levou 11 segundos. Então local 1 minuto e 8 segundos via UPNAI 41 segundos via Grok 11 segundos. Ok, continuando. Agora vamos importar os componentes essenciais da biblioteca Languixer para a sumarização de texto. Inicializar uma instância do LLM da UPNAI. No caso, vou utilizar o chat UPNAI com o GPT4 ou Mini, a versão mais recente. Quanto é peratura em zero? A usar elementos chave, incluem classes para lidar com textos grandes, otimização, construção de prómpotes e técnicas de sumarização. Aqui eu estou configurando o splitter também, com o Tx de meu caracteres, sem sobreposição, e utilizando espaços vírgulas e quebras de linha como separadores. Isso é para garantir que o texto de input seja dividido por esses separadores em vez de quebrar uma palavra ao meio. Configurado, eu vou abrir o arquivo de texto, que foi salvo anteriormente com a transcrição e dividir separado o conteúdo por meio do método splitted text. Onde cada objeto do document do Langchain é inicializado com um conteúdo de um chunk da lista de textos. Então, se eu der uma conferida no indicisero desse docs, tenho um chunk ou a representação de um chunk. De toda a transcrição, captorei para exibir apenas um chunk. Agora vamos usar a sumarização. A abordagem stealth é a mais simples, na qual o todo o texto do vídeo transcrito é usado em um único prómpote. Esse método pode gerar a exceções. Se todo o texto que queremos resumir foi mais longo do que o tamanho do contexto disponível no LLM. E pode não ser a maneira mais eficiente de lidar com grandes quantidades de texto. Por coincidência, um modelo que nós escolhemos de PT4 ou Mini, tem 128 mil tokens de contexto. Então, a espassa o suficiente para fazer esse experimento com essa transcrição. Vamos experimentar com um prómpote template. Esse prómpote vai gerar o resumo em forma de tópicos. Então, eu tenho prómpote template, escreva um resumo conceso com o bullet point do texto abaixo. Aqui, eu vou imputar o conteúdo data na descrição como o contexto e o último prómpote que eu espero com o LLM resolva é o resumo que eu estou pedindo. E agora, vou iniciar a chen de sumarização usando o tipo aqui, chen type, stuff, como tipo de chen. E o prómpote template foi gerado. Então, vou executar. Ok, temos aqui a sumarização de toda a transcrição do vídeo. Lembrando que no link chen, temos a flexibilidade de criar prómpotes personalizados de acordo com as necessidades específicas. Por exemplo, se você deseja o resultado do resumo do vídeo em inglês, você pode implementar um prómpote que instruou o modelo a gerar um resumo no idioma desejado. Esse caso é bem melhor utilizar o LLM do que utilizar a função de translate dentro do próprio whisper. O risco de alocinação na transcrição ou no translate do poder do whisper é bem superior do que tentar traduzir isso aqui para inglês utilizando o GPT4. Tudo o que fizemos até aqui foi seguir esse fluxo e obter essa sumarização final. Agora, então, agora vamos trabalhar na segunda parte, que é seguir esse fluxo, gerar os chunks, transformar em imbarins, passar por um vector database, gerar um prómpote e fazer um rag. Vamos adicionar as transcrições no vector database, eu vou indexar a transcrição no quadrante, e, por meio de um anche encostomizado, faremos perguntas. Sobre a transcrição da transcrição. Então, aqui eu tenho um modelo de imbarins, taxing bar in 3d small, da OpenAI, as variáveis de ambiente com o windpoint, do meu cluster quadrante, no cloud, mas a API key, eu vou criar uma colection chamada whisper e enviar os documentos da transcrição, os chunks. O exaco está, posso conferir aqui, uma nova coleção foi criada chamada whisper e, nela, contém os chunks da nossa transcrição. Então, posso fazer um teste de simularidade, o perguntar o que são o orden barins, fazer uma consulta no quadrante, e com o multiple, eu recebo quatro chunks, mais similares ao que foi perguntado. Agora, eu vou continuar utilizando a GPT4O Mini em um rag com um prómpote um pouco mais elaborado, vamos testar as perguntas e respostas. Então, vou carregar as dependências. E, se eu tenho pletido o prómpote customizado, utilise as transcrições abaixo para responder a pergunta em formato de bullet points e de forma resumida. Se não souber a resposta de higapenas que não sabe, não tem de ventar hoje era uma resposta. Que tem a interpolação para passar o contexto, como parâmetro, ou os chunks que eu vou receber, da minha busca para simularidade do vector database, a pergunta original do usuário, a query ou a consulta que eu vou enviar para esse sistema rag resuma, o orden barins. Ele vai fazer a busca para simularidade, retornar os três chunks mais similares, carregar apenas o conteúdo de texto desses chunks e gerar uma resposta. Ok, como resposta temos o orden barins, representam palavras e textos como vetores numéricos e assim por diante. Bom, hoje é só, espero que você tenha gostado, os links estão na descrição. Muito obrigado por assistir e nos vemos no próximo.